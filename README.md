ollama serve
ollama pull llama3.2:3b

pip install -r requirements.txt

configure api key

python main.py

Server starts at `http://localhost:8000`
